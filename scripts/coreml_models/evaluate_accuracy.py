#!/usr/bin/env python3
"""
Evaluate CoreML FP16 model accuracy against the full FP32 PyTorch model.

This script compares embeddings generated by:
1. Full FP32 PyTorch model (baseline)
2. CoreML FP16 quantized model

Usage:
  python scripts/coreml_models/evaluate_accuracy.py \
    --coreml ./coreml/Qwen3-Embedding-4B-FP16.mlpackage \
    --hf Qwen/Qwen3-Embedding-4B \
    --test-queries "path/to/queries.txt"
"""
import argparse
import time
from pathlib import Path
import numpy as np
import os


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """Compute cosine similarity between two vectors."""
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


def mean_squared_error(a: np.ndarray, b: np.ndarray) -> float:
    """Compute mean squared error between two vectors."""
    return float(np.mean((a - b) ** 2))


def get_fp32_embeddings(model_id: str, texts: list[str]) -> tuple[list[np.ndarray], float]:
    """
    Generate embeddings using full FP32 PyTorch model (baseline).
    
    Returns:
        (embeddings, avg_latency_ms)
    """
    from transformers import AutoModel, AutoTokenizer
    import torch
    
    print(f"ðŸ“Š Loading FP32 baseline model: {model_id}")
    model = AutoModel.from_pretrained(model_id)
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model.eval()
    
    embeddings = []
    latencies = []
    
    max_len = int(os.environ.get("PAPR_COREML_MAX_LENGTH", "32"))
    with torch.no_grad():
        for text in texts:
            start = time.perf_counter()
            
            inputs = tokenizer(
                text,
                return_tensors="pt",
                padding="max_length",
                max_length=max_len,
                truncation=True,
            )
            # Request hidden states to mirror CoreML recipe
            outputs = model(**inputs, output_hidden_states=True)

            hidden_states = None
            if hasattr(outputs, "hidden_states") and outputs.hidden_states is not None:
                layers = outputs.hidden_states[-4:]  # last-N (N=4) to match converter default
                stacked = torch.stack(layers, dim=0)  # [L, B, S, H]
                hidden_states = torch.mean(stacked, dim=0).squeeze(0)  # [S, H]
            else:
                hidden_states = outputs.last_hidden_state.squeeze(0)  # [S, H]

            # Attention-masked mean pooling
            attention_mask = inputs["attention_mask"]  # [1, S]
            mask_1d = attention_mask.squeeze(0).float()  # [S]
            mask_2d = mask_1d.unsqueeze(-1).expand_as(hidden_states)  # [S, H]
            summed = torch.sum(hidden_states * mask_2d, dim=0)  # [H]
            sum_mask = torch.clamp(mask_1d.sum(), min=1e-9)  # scalar
            pooled = summed / sum_mask  # [H]

            # L2 normalization
            pooled = pooled / torch.norm(pooled, p=2)

            embedding = pooled.cpu().numpy()
            
            latency_ms = (time.perf_counter() - start) * 1000
            latencies.append(latency_ms)
            embeddings.append(embedding)
    
    avg_latency = sum(latencies) / len(latencies)
    print(f"   âœ… FP32 avg latency: {avg_latency:.1f}ms per query")
    
    return embeddings, avg_latency


def get_coreml_embeddings(coreml_path: str, tokenizer_id: str, texts: list[str]) -> tuple[list[np.ndarray], float]:
    """
    Generate embeddings using CoreML FP16 model.
    
    Returns:
        (embeddings, avg_latency_ms)
    """
    from transformers import AutoTokenizer
    import coremltools as ct
    
    print(f"ðŸ“Š Loading CoreML FP16 model: {coreml_path}")
    model = ct.models.MLModel(coreml_path)
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)
    
    embeddings = []
    latencies = []
    
    for text in texts:
        start = time.perf_counter()
        
        # Match conversion-time padding
        enc = tokenizer([text], padding="max_length", max_length=32, truncation=True, return_tensors="np")
        inputs = {
            "input_ids": enc["input_ids"].astype(np.int32),
            "attention_mask": enc["attention_mask"].astype(np.int32),
        }
        
        result = model.predict(inputs)
        # CoreML returns dict with output name
        embedding = list(result.values())[0].squeeze()
        embedding = embedding / np.linalg.norm(embedding)
        
        latency_ms = (time.perf_counter() - start) * 1000
        latencies.append(latency_ms)
        embeddings.append(embedding)
    
    avg_latency = sum(latencies) / len(latencies)
    print(f"   âœ… CoreML avg latency: {avg_latency:.1f}ms per query")
    
    return embeddings, avg_latency


def evaluate_accuracy(
    fp32_embeddings: list[np.ndarray],
    fp16_embeddings: list[np.ndarray],
    queries: list[str],
) -> None:
    """Compare FP32 and FP16 embeddings and print accuracy metrics."""
    
    print("\n" + "="*80)
    print("ðŸ“Š ACCURACY EVALUATION RESULTS")
    print("="*80)
    
    cosine_sims = []
    mse_errors = []
    
    for i, (fp32, fp16) in enumerate(zip(fp32_embeddings, fp16_embeddings)):
        cos_sim = cosine_similarity(fp32, fp16)
        mse = mean_squared_error(fp32, fp16)
        
        cosine_sims.append(cos_sim)
        mse_errors.append(mse)
        
        print(f"\nQuery {i+1}: {queries[i][:60]}...")
        print(f"   Cosine similarity: {cos_sim:.6f} ({cos_sim*100:.4f}%)")
        print(f"   MSE: {mse:.8f}")
        print(f"   L2 distance: {np.linalg.norm(fp32 - fp16):.6f}")
    
    # Summary statistics
    avg_cos_sim = sum(cosine_sims) / len(cosine_sims)
    min_cos_sim = min(cosine_sims)
    avg_mse = sum(mse_errors) / len(mse_errors)
    
    print("\n" + "="*80)
    print("ðŸ“ˆ SUMMARY STATISTICS")
    print("="*80)
    print(f"Average cosine similarity: {avg_cos_sim:.6f} ({avg_cos_sim*100:.4f}%)")
    print(f"Minimum cosine similarity: {min_cos_sim:.6f} ({min_cos_sim*100:.4f}%)")
    print(f"Average MSE: {avg_mse:.8f}")
    print(f"Accuracy preservation: {avg_cos_sim*100:.4f}%")
    print(f"Accuracy loss: {(1-avg_cos_sim)*100:.4f}%")
    
    # Quality rating
    if avg_cos_sim >= 0.9999:
        rating = "ðŸŸ¢ EXCELLENT"
        comment = "Negligible accuracy loss (<0.01%)"
    elif avg_cos_sim >= 0.999:
        rating = "ðŸŸ¢ VERY GOOD"
        comment = "Minimal accuracy loss (<0.1%)"
    elif avg_cos_sim >= 0.99:
        rating = "ðŸŸ¡ GOOD"
        comment = "Acceptable accuracy loss (<1%)"
    elif avg_cos_sim >= 0.98:
        rating = "ðŸŸ¡ MODERATE"
        comment = "Noticeable accuracy loss (1-2%)"
    else:
        rating = "ðŸ”´ POOR"
        comment = "Significant accuracy loss (>2%)"
    
    print(f"\nQuality Rating: {rating}")
    print(f"Assessment: {comment}")
    
    # Recommendation
    print("\n" + "="*80)
    print("ðŸ’¡ RECOMMENDATION")
    print("="*80)
    if avg_cos_sim >= 0.999:
        print("âœ… FP16 quantization is highly recommended for production use.")
        print("   Speed gains far outweigh the negligible accuracy loss.")
    elif avg_cos_sim >= 0.99:
        print("âœ… FP16 quantization is acceptable for most use cases.")
        print("   Consider testing on your specific workload.")
    else:
        print("âš ï¸  FP16 quantization may not be suitable for accuracy-critical applications.")
        print("   Consider using full precision or testing thoroughly.")


def main() -> None:
    parser = argparse.ArgumentParser(description="Evaluate CoreML model accuracy")
    parser.add_argument("--coreml", required=True, help="Path to CoreML .mlpackage")
    parser.add_argument("--hf", required=True, help="HuggingFace model ID for baseline")
    parser.add_argument("--test-queries", help="Path to file with test queries (one per line)")
    parser.add_argument("--num-samples", type=int, default=50, help="Number of test samples")
    
    args = parser.parse_args()
    
    # Load test queries
    if args.test_queries and Path(args.test_queries).exists():
        with open(args.test_queries) as f:
            queries = [line.strip() for line in f if line.strip()][:args.num_samples]
    else:
        # Default test queries covering various use cases
        queries = [
            "What is machine learning?",
            "How to install Python packages",
            "Best practices for API design",
            "Explain quantum computing simply",
            "Natural language processing tutorial",
            "Docker container optimization",
            "React component lifecycle methods",
            "SQL query performance tuning",
            "Git merge vs rebase difference",
            "Kubernetes deployment strategies",
            "REST API vs GraphQL comparison",
            "Python async await explained",
            "TensorFlow vs PyTorch features",
            "Microservices architecture patterns",
            "Database indexing best practices",
            "OAuth 2.0 authentication flow",
            "Memory management in C++",
            "JavaScript promises and async",
            "Linux system administration basics",
            "Network security fundamentals",
        ][:args.num_samples]
    
    print(f"ðŸ§ª Testing with {len(queries)} queries\n")
    
    # Generate embeddings with both models
    fp32_embeddings, fp32_latency = get_fp32_embeddings(args.hf, queries)
    fp16_embeddings, fp16_latency = get_coreml_embeddings(args.coreml, args.hf, queries)
    
    # Speed comparison
    speedup = fp32_latency / fp16_latency
    print(f"\nâš¡ Speed comparison:")
    print(f"   FP32: {fp32_latency:.1f}ms")
    print(f"   FP16: {fp16_latency:.1f}ms")
    print(f"   Speedup: {speedup:.2f}x faster")
    
    # Evaluate accuracy
    evaluate_accuracy(fp32_embeddings, fp16_embeddings, queries)


if __name__ == "__main__":
    main()

